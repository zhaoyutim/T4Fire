
2021-12-03 13:22:53.044941: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-12-03 13:22:56.846799: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Model: "vit-b16"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 10, 45)]     0
__________________________________________________________________________________________________
patch_encoder (PatchEncoder)    (10, 768)            7680        input_1[0][0]
__________________________________________________________________________________________________
dense (Dense)                   (None, 10, 768)      35328       input_1[0][0]
__________________________________________________________________________________________________
tf.__operators__.add (TFOpLambd (None, 10, 768)      0           patch_encoder[0][0]
                                                                 dense[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_0 (Tra ((None, 10, 768), (N 7087872     tf.__operators__.add[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_1 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_0[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_2 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_1[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_3 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_2[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_4 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_3[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_5 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_4[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_6 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_5[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_7 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_6[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_8 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_7[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_9 (Tra ((None, 10, 768), (N 7087872     Transformer/encoderblock_8[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_10 (Tr ((None, 10, 768), (N 7087872     Transformer/encoderblock_9[0][0]
__________________________________________________________________________________________________
Transformer/encoderblock_11 (Tr ((None, 10, 768), (N 7087872     Transformer/encoderblock_10[0][0]
__________________________________________________________________________________________________
Transformer/encoder_norm (Layer (None, 10, 768)      1536        Transformer/encoderblock_11[0][0]
__________________________________________________________________________________________________
head (Dense)                    (None, 10, 2)        1538        Transformer/encoder_norm[0][0]
==================================================================================================
Total params: 85,100,546
Trainable params: 85,100,546
Non-trainable params: 0
__________________________________________________________________________________________________
training in progress
Epoch 1/50








